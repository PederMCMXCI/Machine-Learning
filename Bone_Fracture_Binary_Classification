


#model = InceptionV3Model(num_classes=len(train_dataset.classes))
#optimizer = optim.Adam(model.parameters(), lr=0.001)
#criterion = nn.CrossEntropyLoss(label_smoothing=0.4)
#mlflow.log_param("label_smoothing", 0.4)  # Explizit label_smoothing loggen
#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 30, 50], gamma=0.1)

# Die Umgebung basiert auf einem Docker-Container mit PyTorch und wird in Visual Studio Code genutzt.
# Ziel ist es, präzise Metriken wie Genauigkeit, Präzision, Recall und F1-Score zu berechnen.

# Train Metrics - Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9997, F1 Score: 0.9997
# Validation Metrics - Accuracy: 0.9915, Precision: 0.9916, Recall: 0.9915, F1 Score: 0.9915
# Classification Report (Validation):

            #   precision    recall  f1-score   support

   # fractured       1.00      0.98      0.99       337
#not fractured       0.99      1.00      0.99       486

     #accuracy                           0.99       823
   # macro avg       0.99      0.99      0.99       823
 #weighted avg       0.99      0.99      0.99       823

# Test Metrics - Accuracy: 98.2000, Precision: 0.9822, Recall: 0.9820, F1 Score: 0.9820
# Test Classification Report:

              # precision    recall  f1-score   support

    #fractured       0.99      0.97      0.98       238
#not fractured       0.97      0.99      0.98       262

    # accuracy                           0.98       500
    #macro avg       0.98      0.98      0.98       500
 #weighted avg       0.98      0.98      0.98       500


import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import os
import sys
import pandas as pd
from pytorchcv.model_provider import get_model as ptcv_get_model
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score
import torchvision.models as models
import mlflow
import mlflow.pytorch

# Setze die Tracking-URI und das gewünschte Experiment
mlflow.set_tracking_uri("file:///E:/Machine_Learning/Bone_Fracture_Binary_Classification/mlruns")
experiment_name = "Bone_Fracture_Binary_Classification"
mlflow.set_experiment(experiment_name)

# Debug: Überprüfe, ob das Experiment richtig gesetzt ist
experiment = mlflow.get_experiment_by_name(experiment_name)
print(f"MLflow Experiment gesetzt: {experiment.name} (ID: {experiment.experiment_id})")

# Setze das Gerät auf CUDA, wenn verfügbar
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Data Augmentation und Normalisierung
transform = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.RandomResizedCrop(299, scale=(0.8, 1.0)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

def get_script_directory():
    if getattr(sys, 'frozen', False):
        script_dir = sys._MEIPASS
    else:
        script_dir = os.path.dirname(os.path.abspath(__file__))
    return script_dir

def load_datasets():
    script_dir = get_script_directory()
    parent_dir = os.path.dirname(script_dir)

    train_dir = os.path.join(parent_dir, "Bone_Fracture_Binary_Classification", "train")
    valid_dir = os.path.join(parent_dir, "Bone_Fracture_Binary_Classification", "val")
    test_dir = os.path.join(parent_dir, "Bone_Fracture_Binary_Classification", "test")

    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)
    valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)
    test_dataset  = datasets.ImageFolder(root=test_dir, transform=transform)

    return train_dataset, valid_dataset, test_dataset

# Lade die Datensätze
train_dataset, valid_dataset,test_dataset  = load_datasets()

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)
test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)  # Test-Dataset auskommentiert

class InceptionV3Model(nn.Module):
    def __init__(self, num_classes):
        super(InceptionV3Model, self).__init__()
        inception_v3 = models.inception_v3(pretrained=True)
        # Passe die letzte Fully Connected Layer an
        in_features = inception_v3.fc.in_features
        inception_v3.fc = nn.Linear(in_features, num_classes)
        self.model = inception_v3

    def forward(self, x):
        outputs = self.model(x)
        if isinstance(outputs, tuple):  # Falls das Modell mehrere Outputs zurückgibt
            return outputs[0]  # Nur den Hauptausgang zurückgeben
        return outputs

def train_and_validate(model, device, train_loader, valid_loader, test_loader, criterion, optimizer, scheduler, num_epochs=60):
    model.to(device)
    mlflow.log_params({
        "num_epochs": num_epochs,
        "batch_size": 32,
        "learning_rate": optimizer.param_groups[0]['lr'],
        "criterion": str(criterion),
        "scheduler": str(scheduler)
    })

    for epoch in range(num_epochs):
        # Training
        model.train()
        total_train_loss = 0
        train_labels = []
        train_preds = []

        progress_bar_train = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Training]', leave=True)
        for images, labels in progress_bar_train:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            train_labels.extend(labels.detach().cpu().numpy())
            train_preds.extend(predicted.detach().cpu().numpy())

            train_accuracy = accuracy_score(train_labels, train_preds) * 100
            progress_bar_train.set_postfix(
                train_accuracy=f"{train_accuracy:.1f}%",
                train_loss=f"{total_train_loss / len(train_loader):.4f}"
            )

        train_loss = total_train_loss / len(train_loader)
        train_accuracy = accuracy_score(train_labels, train_preds) * 100
        train_precision = precision_score(train_labels, train_preds, average='weighted', zero_division=1)
        train_recall = recall_score(train_labels, train_preds, average='weighted', zero_division=1)
        train_f1 = f1_score(train_labels, train_preds, average='weighted', zero_division=1)

        mlflow.log_metrics({
            "train_loss": train_loss,
            "train_accuracy": train_accuracy,
            "train_precision": train_precision,
            "train_recall": train_recall,
            "train_f1": train_f1
        }, step=epoch)

        # Validierung
        model.eval()
        total_valid_loss = 0
        valid_labels = []
        valid_preds = []

        progress_bar_valid = tqdm(valid_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [Validation]', leave=True)
        with torch.no_grad():
            for images, labels in progress_bar_valid:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                total_valid_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                valid_labels.extend(labels.detach().cpu().numpy())
                valid_preds.extend(predicted.detach().cpu().numpy())

                valid_accuracy = accuracy_score(valid_labels, valid_preds) * 100
                progress_bar_valid.set_postfix(
                    valid_accuracy=f"{valid_accuracy:.1f}%",
                    valid_loss=f"{total_valid_loss / len(valid_loader):.4f}"
                )

        valid_loss = total_valid_loss / len(valid_loader)
        valid_accuracy = accuracy_score(valid_labels, valid_preds) * 100
        valid_precision = precision_score(valid_labels, valid_preds, average='weighted', zero_division=1)
        valid_recall = recall_score(valid_labels, valid_preds, average='weighted', zero_division=1)
        valid_f1 = f1_score(valid_labels, valid_preds, average='weighted', zero_division=1)

        mlflow.log_metrics({
            "valid_loss": valid_loss,
            "valid_accuracy": valid_accuracy,
            "valid_precision": valid_precision,
            "valid_recall": valid_recall,
            "valid_f1": valid_f1
        }, step=epoch)

        # Scheduler updaten
        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(valid_loss)
        else:
            scheduler.step()

        tqdm.write(
            f"Epoch {epoch + 1}/{num_epochs}: "
            f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.1f}%, "
            f"Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.1f}%\n"
        )

    # Finale Train- und Validierungsmetriken
    final_train_accuracy = accuracy_score(train_labels, train_preds)
    final_train_precision = precision_score(train_labels, train_preds, average='weighted', zero_division=1)
    final_train_recall = recall_score(train_labels, train_preds, average='weighted', zero_division=1)
    final_train_f1 = f1_score(train_labels, train_preds, average='weighted', zero_division=1)

    final_valid_accuracy = accuracy_score(valid_labels, valid_preds)
    final_valid_precision = precision_score(valid_labels, valid_preds, average='weighted', zero_division=1)
    final_valid_recall = recall_score(valid_labels, valid_preds, average='weighted', zero_division=1)
    final_valid_f1 = f1_score(valid_labels, valid_preds, average='weighted', zero_division=1)

    class_report = classification_report(valid_labels, valid_preds, target_names=train_dataset.classes)

    print("\n# Train Metrics - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}".format(
        final_train_accuracy, final_train_precision, final_train_recall, final_train_f1))
    print("# Validation Metrics - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}".format(
        final_valid_accuracy, final_valid_precision, final_valid_recall, final_valid_f1))
    print("# Classification Report (Validation):\n")
    print(class_report)

    with open("classification_report.txt", "w") as f:
        f.write(class_report)
    mlflow.log_artifact("classification_report.txt")

    # --- Testphase (auskommentiert) ---

    model.eval()
    total_test_loss = 0
    test_labels = []
    test_preds = []

    progress_bar_test = tqdm(test_loader, desc='Testing', leave=True)
    with torch.no_grad():
        for images, labels in progress_bar_test:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_test_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            test_labels.extend(labels.detach().cpu().numpy())
            test_preds.extend(predicted.detach().cpu().numpy())

            test_accuracy = accuracy_score(test_labels, test_preds) * 100
            progress_bar_test.set_postfix(
                test_accuracy=f"{test_accuracy:.1f}%",
                test_loss=f"{total_test_loss / len(test_loader):.4f}"
            )

    test_loss = total_test_loss / len(test_loader)
    test_accuracy = accuracy_score(test_labels, test_preds) * 100
    test_precision = precision_score(test_labels, test_preds, average='weighted', zero_division=1)
    test_recall = recall_score(test_labels, test_preds, average='weighted', zero_division=1)
    test_f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=1)
    test_class_report = classification_report(test_labels, test_preds, target_names=train_dataset.classes)

    print("\n# Test Metrics - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}".format(
        test_accuracy, test_precision, test_recall, test_f1))
    print("# Test Classification Report:\n")
    print(test_class_report)

    mlflow.log_metrics({
        "test_loss": test_loss,
        "test_accuracy": test_accuracy,
        "test_precision": test_precision,
        "test_recall": test_recall,
        "test_f1": test_f1
    })

    with open("test_classification_report.txt", "w") as f:
        f.write(test_class_report)
    mlflow.log_artifact("test_classification_report.txt")


with mlflow.start_run():
    # Modell, Optimierer und Scheduler definieren
    model = InceptionV3Model(num_classes=len(train_dataset.classes))
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.4)
    mlflow.log_param("label_smoothing", 0.4)  # Explizit label_smoothing loggen
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 30, 50], gamma=0.1)

    # Modelltraining starten
    # Der Parameter test_loader wird weiterhin übergeben, da er eventuell in anderen Teilen gebraucht werden könnte.
    train_and_validate(model, device, train_loader, valid_loader, test_loader, criterion, optimizer, scheduler, num_epochs=30)

